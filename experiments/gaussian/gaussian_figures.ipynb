{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "num_subnet_arr = [2, 4, 8, 16]\n",
    "sigmas = torch.linspace(0,5,500)\n",
    "ideal_coverages = norm.cdf(sigmas) - norm.cdf(-sigmas)\n",
    "\n",
    "part_llr_pred = torch.load('YOUR_DIR_HERE/partition_llr_pred_arr.pt')\n",
    "part_llr_true = torch.load('YOUR_DIR_HERE/partition_llr_true_arr.pt')\n",
    "part_llr_unc = torch.load('YOUR_DIR_HERE/partition_llr_unc_arr.pt')\n",
    "\n",
    "# trained up to 32 for bootstrap for bias checking purposes, so drop this training for fig 1\n",
    "strap_llr_pred = torch.load('YOUR_DIR_HERE/bootstrap_llr_pred_arr.pt')[:-1]\n",
    "strap_llr_true = torch.load('YOUR_DIR_HERE/bootstrap_llr_true_arr.pt')[:-1]\n",
    "strap_llr_unc = torch.load('YOUR_DIR_HERE/bootstrap_llr_unc_arr.pt')[:-1]\n",
    "\n",
    "num_trainings = part_llr_pred.shape[1]\n",
    "\n",
    "part_z_scores = ((part_llr_pred - part_llr_true)/part_llr_unc)\n",
    "part_coverages = (part_z_scores.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)\n",
    "\n",
    "strap_z_scores = ((strap_llr_pred - strap_llr_true)/strap_llr_unc)\n",
    "strap_coverages = (strap_z_scores.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)\n",
    "\n",
    "color_pal = sns.color_palette('bright')\n",
    "mpl.rc('font',family='Times New Roman')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "num_subnet_arr = [2, 4, 8, 16]\n",
    "coverages = torch.stack([part_coverages[:4], strap_coverages[:4]], dim=-1)\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(5, 2, height_ratios=[2.5, 1, 0.5, 2.5, 1], hspace=0.04, wspace=0)\n",
    "\n",
    "axs = np.array([\n",
    "    [fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1])],\n",
    "    [fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1])],\n",
    "    [None, None],  # This row is an empty space\n",
    "    [fig.add_subplot(gs[3, 0]), fig.add_subplot(gs[3, 1])],\n",
    "    [fig.add_subplot(gs[4, 0]), fig.add_subplot(gs[4, 1])],\n",
    "])\n",
    "# fig, axs = plt.subplots(4,2,sharex=True,figsize = (16,12),gridspec_kw={'hspace': 0, 'wspace': 0, 'height_ratios': [2.5, 1, 2.5, 1]})\n",
    "axs[0,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[0,1].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[1,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[1,0].tick_params(axis='x', which='major', direction='in', labelsize=0)\n",
    "axs[1,1].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[3,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[3,1].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[4,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[4,1].tick_params(axis='y', which='major', direction='in', labelsize=0)\n",
    "axs[4,1].tick_params(axis='x', which='major', direction='in', labelsize=16)\n",
    "\n",
    "method_names = ['Partition', 'Bootstrap']\n",
    "colors = [color_pal[0], color_pal[1]]\n",
    "axs[0,0].set_title(f\"$M=2$\", fontsize=22)\n",
    "axs[0,0].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[1,0].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,0].set_ylabel(r\"$c(z) - \\bar{c}(z)$\", fontsize=18)\n",
    "axs[1,0].set_ylim(-0.2, 0.2)\n",
    "axs[0,0].set_ylabel(r'$c(z)$',fontsize=18)\n",
    "\n",
    "axs[0,1].set_title(f\"$M=4$\", fontsize=22)\n",
    "axs[0,1].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[1,1].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,1].set_ylim(-0.2, 0.2)\n",
    "\n",
    "axs[3,0].set_title(f\"$M=8$\", fontsize=22)\n",
    "axs[3,0].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[4,0].axhline(y=0, color='k', linestyle='--')\n",
    "axs[4,0].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[4,0].set_ylabel(r\"$c(z) - \\bar{c}(z)$\", fontsize=18)\n",
    "axs[4,0].set_ylim(-0.1, 0.1)\n",
    "axs[3,0].set_ylabel(r'$c(z)$',fontsize=18)\n",
    "\n",
    "axs[3,1].set_title(f\"$M=16$\", fontsize=22)\n",
    "axs[3,1].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[4,1].axhline(y=0, color='k', linestyle='--')\n",
    "axs[4,1].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[4,1].set_ylim(-0.1, 0.1)\n",
    "\n",
    "for method_idx_switch in range(len(method_names)):\n",
    "    for subnet_idx, num_subnets in enumerate(num_subnet_arr):\n",
    "        method_idx = int(1-method_idx_switch)\n",
    "        if subnet_idx == 0:\n",
    "            axs[0,0].plot(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[0,0].fill_between(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[subnet_idx,:,:,method_idx].mean(axis=0) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "            axs[1,0].plot(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[1,0].fill_between(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "        elif subnet_idx == 1:\n",
    "            axs[0,1].plot(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[0,1].fill_between(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[subnet_idx,:,:,method_idx].mean(axis=0) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "            axs[1,1].plot(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[1,1].fill_between(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "        elif subnet_idx == 2:\n",
    "            axs[3,0].plot(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[3,0].fill_between(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[subnet_idx,:,:,method_idx].mean(axis=0) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "            axs[4,0].plot(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[4,0].fill_between(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "        else:\n",
    "            axs[3,1].plot(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[3,1].fill_between(sigmas, coverages[subnet_idx,:,:,method_idx].mean(axis=0) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[subnet_idx,:,:,method_idx].mean(axis=0) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "            axs[4,1].plot(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx])\n",
    "            axs[4,1].fill_between(sigmas, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[subnet_idx,:,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[subnet_idx,:,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx])\n",
    "\n",
    "axs[0,0].legend(fontsize=16, frameon=False)\n",
    "axs[0,1].legend(fontsize=16, frameon=False)\n",
    "axs[3,0].legend(fontsize=16, frameon=False)\n",
    "axs[3,1].legend(fontsize=16, frameon=False)\n",
    "plt.suptitle(\"Gaussian Case Study, Coverage on Log Likelihood Ratio\", fontsize=24)\n",
    "plt.savefig(\"fig1.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This one takes awhile (~20 minutes on my machine) since it trains the networks from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchmin import minimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "num_samples = 50000\n",
    "train_frac = 0.5\n",
    "num_mixture_samples = int(train_frac * num_samples)\n",
    "num_train_samples = int(train_frac * num_samples)\n",
    "num_val_samples = num_samples - num_train_samples\n",
    "\n",
    "train_batch_size = 5000\n",
    "val_batch_size = 1*train_batch_size\n",
    "num_subnets_arr = [2]\n",
    "\n",
    "num_runs = 1\n",
    "num_trainings = 1\n",
    "units = 32\n",
    "latent_dim = 32\n",
    "\n",
    "llr_true_arr = torch.zeros((len(num_subnets_arr), num_trainings, num_runs))\n",
    "llr_pred_arr = torch.zeros((len(num_subnets_arr), num_trainings, num_runs))\n",
    "llr_unc_arr = torch.zeros((len(num_subnets_arr), num_trainings, num_runs))\n",
    "\n",
    "true_arr = torch.zeros((len(num_subnets_arr), num_trainings, num_runs, 3))\n",
    "pred_arr = torch.zeros((len(num_subnets_arr), num_trainings, num_runs, 3))\n",
    "unc_arr = torch.zeros((len(num_subnets_arr), num_trainings, num_runs, 3))\n",
    "    \n",
    "def model_with_weights_scaled(submodels, dataloader, w):\n",
    "    dataset_length = len(dataloader.dataset)\n",
    "    h_outs = torch.zeros(dataset_length, device=device)\n",
    "    end_idx = 0\n",
    "    for data in dataloader:\n",
    "        start_idx = end_idx\n",
    "        end_idx += len(data[0])\n",
    "        h_outs[start_idx:end_idx] = submodels.submodel_all(data[0].to(device))@w\n",
    "    return h_outs\n",
    "\n",
    "def mlc_min(w, submodels, qloader, ploader):\n",
    "    q_out = model_with_weights_scaled(submodels, qloader, w)\n",
    "    p_out = model_with_weights_scaled(submodels, ploader, w)\n",
    "    mlc1 = -(q_out.mean() - (torch.exp(p_out) - 1).mean())\n",
    "    mlc2 = -(-p_out.mean() - (torch.exp(-q_out) - 1).mean())\n",
    "    return mlc1 + mlc2\n",
    "\n",
    "def MLC(y_true, model_outputs, net_idx):\n",
    "    w00 = torch.zeros(model_outputs.shape[-1], device=device)\n",
    "    w00[net_idx] = 1.\n",
    "    y_pred = (model_outputs@w00).unsqueeze(1)\n",
    "    cont1 = -(y_true.unsqueeze(1) * y_pred)\n",
    "    cont2 = -(1-y_true.unsqueeze(1)) * (1 - torch.exp(y_pred))\n",
    "    cont3 = -(1-y_true.unsqueeze(1))*(-y_pred)\n",
    "    cont4 = -(y_true.unsqueeze(1)) * (1 - torch.exp(-y_pred))\n",
    "    return cont1+cont2+cont3+cont4\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_subnets):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_subnets = num_subnets\n",
    "\n",
    "        self.layer1 = nn.Linear(1, units)\n",
    "        self.layer2 = nn.Linear(units, units)\n",
    "        self.layer3 = nn.Linear(units, 1)\n",
    "\n",
    "        self.layer1_list = nn.ModuleList([nn.Linear(1, units) for i in range(self.num_subnets)])\n",
    "        self.layer2_list = nn.ModuleList([nn.Linear(units, units) for i in range(self.num_subnets)])\n",
    "        self.layer3_list = nn.ModuleList([nn.Linear(units, 1) for i in range(self.num_subnets)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.layer1(x), negative_slope = 0.2)\n",
    "        x = F.leaky_relu(self.layer2(x), negative_slope = 0.2)\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "    def submodel_all(self, x):\n",
    "        x1 = [F.leaky_relu(self.layer1_list[i](x), negative_slope = 0.2) for i in range(self.num_subnets)]\n",
    "        x1 = [F.leaky_relu(self.layer2_list[i](x1[i]), negative_slope = 0.2) for i in range(self.num_subnets)]\n",
    "        x1 = [self.layer3_list[i](x1[i]) for i in range(self.num_subnets)]\n",
    "        x1 = torch.cat(x1, axis=1)\n",
    "        x1 = torch.cat([x1, torch.ones(x1.shape[0], device=device).unsqueeze(1)], axis=1)\n",
    "        return x1\n",
    "\n",
    "def train_func(model_to_train, epochs, num_subnets):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    opt = optim.Adam(model_to_train.parameters(), lr=1e-2)\n",
    "    for start_idx in range(num_subnets):\n",
    "        train_strap = np.random.randint(0, x_train.shape[0], x_train.shape[0])\n",
    "        x_train_strap = x_train[train_strap]\n",
    "        y_train_strap = y_train[train_strap]\n",
    "        val_strap = np.random.randint(0, x_val.shape[0], x_val.shape[0])\n",
    "        x_val_strap = x_val[val_strap]\n",
    "        y_val_strap = y_val[val_strap]\n",
    "\n",
    "        trainset_strap = torch.utils.data.TensorDataset(x_train_strap, y_train_strap)\n",
    "        valset_strap = torch.utils.data.TensorDataset(x_val_strap, y_val_strap)\n",
    "        trainloader_strap = torch.utils.data.DataLoader(trainset_strap, batch_size=train_batch_size, shuffle=True)\n",
    "        valloader_strap = torch.utils.data.DataLoader(valset_strap, batch_size=val_batch_size, shuffle=False)\n",
    "        for param in model_to_train.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model_to_train.layer1_list[start_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model_to_train.layer2_list[start_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in model_to_train.layer3_list[start_idx].parameters():\n",
    "            param.requires_grad = True\n",
    "        print(f\"Training basis function {start_idx}\", flush=True)\n",
    "        min_val_loss = 1e10\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "            batches = 0\n",
    "            for i, data in enumerate(trainloader_strap):\n",
    "                batches += 1\n",
    "                opt.zero_grad()\n",
    "                inputs = data[0].to(device)\n",
    "                train_outputs = model_to_train.submodel_all(inputs)\n",
    "                loss = MLC(data[1].to(device), train_outputs, start_idx).mean()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                running_loss += loss.item()\n",
    "            val_batches = 0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(valloader_strap):\n",
    "                    val_batches += 1\n",
    "                    inputs = data[0].to(device)\n",
    "                    val_outputs = model_to_train.submodel_all(inputs)\n",
    "                    val_loss += MLC(data[1].to(device), val_outputs, start_idx).mean().item()\n",
    "            train_losses.append(running_loss/batches)\n",
    "            val_losses.append(val_loss/val_batches)\n",
    "            if val_loss/val_batches < min_val_loss:\n",
    "                min_val_loss = val_loss/val_batches\n",
    "                best_model = model_to_train.state_dict()\n",
    "            print(f\"Epoch {epoch+1} train loss: {running_loss/batches}, val loss: {val_loss/val_batches}\", flush=True)\n",
    "        model_to_train.load_state_dict(best_model)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def neg_maximum_likelihood_f(f, model_outputs):\n",
    "    #need to fix this implementation to allow for difference in sizes of data and prior\n",
    "    return -torch.log(torch.exp(model_outputs)*f + (1-f)).sum()\n",
    "\n",
    "def neg_maximum_likelihood_f_wrapper(*args):\n",
    "    return neg_maximum_likelihood_f(*args).detach().cpu().numpy()\n",
    "\n",
    "def calc_ai(f, model_outputs_all, w):\n",
    "    model_outputs = model_outputs_all@w\n",
    "    ai = (((torch.exp(model_outputs)/(torch.exp(model_outputs)*f + (1-f))**2)).unsqueeze(1)*model_outputs_all).sum(axis=0)\n",
    "    return ai.detach()\n",
    "\n",
    "def calc_second_deriv(f, model_outputs_all, w):\n",
    "    model_outputs = model_outputs_all@w\n",
    "    second_deriv = (-(torch.exp(model_outputs)-1)**2/(torch.exp(model_outputs)*f + (1-f))**2).sum()\n",
    "    return second_deriv.detach()\n",
    "\n",
    "def uncertainties(f, model_outputs_all, w, cov):\n",
    "    ai = calc_ai(f, model_outputs_all, w)\n",
    "    second_deriv = calc_second_deriv(f, model_outputs_all, w)\n",
    "    r = torch.abs(1/second_deriv) / (1/second_deriv**2 * ai@cov@ai)\n",
    "    return torch.abs(1/second_deriv) + 1/second_deriv**2 * (ai.double()@cov.double()@ai.double()).float(), r\n",
    "\n",
    "for subnet_idx, num_subnets in enumerate(num_subnets_arr):\n",
    "    print(\"Number of subnets is\", num_subnets, flush=True)\n",
    "    for training in range(num_trainings):\n",
    "        if training%100 == 0:\n",
    "            print(\"Starting training run\", training, flush=True)\n",
    "        qs = torch.randn(num_samples) + .1\n",
    "        ps = torch.randn(num_samples) - .1\n",
    "        qs_train = qs[0:num_train_samples]\n",
    "        qs_val = qs[num_train_samples:]\n",
    "        ps_train = ps[0:num_train_samples]\n",
    "        ps_val = ps[num_train_samples:]\n",
    "\n",
    "        data_train = torch.concatenate((qs_train,ps_train)).detach()\n",
    "        data_val = torch.concatenate((qs_val,ps_val)).detach()\n",
    "        train_perm_key = torch.randperm(2*num_train_samples).detach()\n",
    "        val_perm_key = torch.randperm(2*num_val_samples).detach()\n",
    "        x_train = data_train[train_perm_key].unsqueeze(1).detach()\n",
    "        x_val = data_val[val_perm_key].unsqueeze(1).detach()\n",
    "        y_train = torch.concatenate((torch.ones(num_train_samples), torch.zeros(num_train_samples)))[train_perm_key].detach()\n",
    "        y_val = torch.concatenate((torch.ones(num_val_samples), torch.zeros(num_val_samples)))[val_perm_key].detach()\n",
    "\n",
    "        qset = torch.utils.data.TensorDataset(x_train[y_train==1])\n",
    "        qloader = torch.utils.data.DataLoader(qset, batch_size = train_batch_size, shuffle=False)\n",
    "        qset_val = torch.utils.data.TensorDataset(x_val[y_val==1])\n",
    "        qloader_val = torch.utils.data.DataLoader(qset_val, batch_size = val_batch_size, shuffle=False)\n",
    "            \n",
    "        pset = torch.utils.data.TensorDataset(x_train[y_train==0])\n",
    "        ploader = torch.utils.data.DataLoader(pset, batch_size = train_batch_size, shuffle=False)\n",
    "        pset_val = torch.utils.data.TensorDataset(x_val[y_val==0])\n",
    "        ploader_val = torch.utils.data.DataLoader(pset_val, batch_size = val_batch_size, shuffle=False)\n",
    "\n",
    "        model = Model(num_subnets)\n",
    "        model.to(device)\n",
    "        train_losses, val_losses = train_func(model, 150, num_subnets)\n",
    "        for run in range(num_runs):\n",
    "            if run%10 == 0:\n",
    "                print(\"Starting run\", run, flush=True)\n",
    "            qs = torch.randn(num_samples) + .1\n",
    "            ps = torch.randn(num_samples) - .1\n",
    "            qs_train = qs[0:num_train_samples]\n",
    "            qs_val = qs[num_train_samples:]\n",
    "            ps_train = ps[0:num_train_samples]\n",
    "            ps_val = ps[num_train_samples:]\n",
    "\n",
    "            data_train = torch.concatenate((qs_train,ps_train)).detach()\n",
    "            data_val = torch.concatenate((qs_val,ps_val)).detach()\n",
    "            train_perm_key = torch.randperm(2*num_train_samples).detach()\n",
    "            val_perm_key = torch.randperm(2*num_val_samples).detach()\n",
    "            x_train = data_train[train_perm_key].unsqueeze(1).detach()\n",
    "            x_val = data_val[val_perm_key].unsqueeze(1).detach()\n",
    "            y_train = torch.concatenate((torch.ones(num_train_samples), torch.zeros(num_train_samples)))[train_perm_key].detach()\n",
    "            y_val = torch.concatenate((torch.ones(num_val_samples), torch.zeros(num_val_samples)))[val_perm_key].detach()\n",
    "\n",
    "            qset = torch.utils.data.TensorDataset(x_train[y_train==1])\n",
    "            qloader = torch.utils.data.DataLoader(qset, batch_size = train_batch_size, shuffle=False)\n",
    "            qset_val = torch.utils.data.TensorDataset(x_val[y_val==1])\n",
    "            qloader_val = torch.utils.data.DataLoader(qset_val, batch_size = val_batch_size, shuffle=False)\n",
    "                \n",
    "            pset = torch.utils.data.TensorDataset(x_train[y_train==0])\n",
    "            ploader = torch.utils.data.DataLoader(pset, batch_size = train_batch_size, shuffle=False)\n",
    "            pset_val = torch.utils.data.TensorDataset(x_val[y_val==0])\n",
    "            ploader_val = torch.utils.data.DataLoader(pset_val, batch_size = val_batch_size, shuffle=False)\n",
    "            w00 = torch.zeros(num_subnets+1, device=device)\n",
    "            w00[0] = 1.\n",
    "            res_root = minimize(lambda w: mlc_min(w, model, qloader, ploader), x0 = w00, method='newton-exact')\n",
    "            w0 = (res_root.x)\n",
    "            val_mlc = mlc_min(w0, model, qloader_val, ploader_val).detach()\n",
    "            model_q_points = torch.zeros((num_subnets+1, qloader.dataset.tensors[0].shape[0]))\n",
    "            model_p_points = torch.zeros((num_subnets+1, ploader.dataset.tensors[0].shape[0]))\n",
    "            q_wgt_points = model_with_weights_scaled(model, qloader, w0).detach().cpu()\n",
    "            p_wgt_points = model_with_weights_scaled(model, ploader, w0).detach().cpu()\n",
    "\n",
    "            end_idx = 0\n",
    "            for data in ploader:\n",
    "                model_prod_points_batch = model.submodel_all(data[0].to(device)).detach()\n",
    "                end_idx += len(model_prod_points_batch)\n",
    "                start_idx = end_idx - len(model_prod_points_batch)\n",
    "                model_p_points[:, start_idx:end_idx] = model_prod_points_batch.cpu().T\n",
    "                \n",
    "            end_idx = 0\n",
    "            for data in qloader:\n",
    "                model_joint_points_batch = model.submodel_all(data[0].to(device)).detach()\n",
    "                end_idx += len(model_joint_points_batch)\n",
    "                start_idx = end_idx - len(model_joint_points_batch)\n",
    "                model_q_points[:, start_idx:end_idx] = model_joint_points_batch.cpu().T\n",
    "\n",
    "            dijq = torch.zeros((num_subnets+1, num_subnets+1))\n",
    "            dijp = torch.zeros((num_subnets+1, num_subnets+1))\n",
    "            cijq = torch.zeros((num_subnets+1, num_subnets+1))\n",
    "            cijp = torch.zeros((num_subnets+1, num_subnets+1))\n",
    "\n",
    "            for i in range(num_subnets+1):\n",
    "                for j in range(num_subnets+1):\n",
    "                    hi_q = model_q_points[i,:]*(1 + torch.exp(-q_wgt_points))\n",
    "                    hj_q = model_q_points[j,:]*(1 + torch.exp(-q_wgt_points))\n",
    "                    dijq[i,j] = (hi_q*hj_q).mean() - (hi_q).mean()*(hj_q).mean()\n",
    "\n",
    "                    hi_p = model_p_points[i,:]*(1 + torch.exp(p_wgt_points))\n",
    "                    hj_p = model_p_points[j,:]*(1 + torch.exp(p_wgt_points))\n",
    "                    dijp[i,j] = (hi_p*hj_p).mean() - (hi_p).mean()*(hj_p).mean()\n",
    "                    cijp[i,j] = -(model_p_points[i,:]*model_p_points[j,:]*torch.exp(p_wgt_points)).mean()\n",
    "                    cijq[i,j] = -(model_q_points[i,:]*model_q_points[j,:]*torch.exp(-q_wgt_points)).mean()\n",
    "            cij = cijq + cijp\n",
    "            dij = dijq + dijp\n",
    "            cov_mat = torch.linalg.solve(cij.double(),torch.linalg.solve(cij.double(), dij.double()).T).float()/end_idx\n",
    "            cov_mat = cov_mat.to(device)\n",
    "            print(\"Cov mat is\", cov_mat, flush=True)\n",
    "            print(\"Weights are\", w0, flush=True)\n",
    "\n",
    "mpl.rc('font',family='Times New Roman')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "color_pal = sns.color_palette('bright')\n",
    "test_pts = torch.linspace(-3,3,1000).unsqueeze(1).to(device)\n",
    "fi = model.submodel_all(test_pts)\n",
    "prediction = fi@w0\n",
    "var = fi@cov_mat@fi.T\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize=(8,6), sharex=True, gridspec_kw={'hspace': 0.04, 'height_ratios': [3, 1]})\n",
    "axs[0].tick_params(direction='in')\n",
    "axs[1].tick_params(direction='in')\n",
    "axs[0].tick_params(labelsize=16)\n",
    "axs[1].tick_params(labelsize=16)\n",
    "axs[1].set_xlabel(r\"$x$\", fontsize=16)\n",
    "axs[1].set_ylabel(r\"$\\log \\hat{r}(x) - \\log r(x)$\", fontsize=16)\n",
    "axs[0].set_ylabel(r\"$\\log \\hat{r}(x)$\", fontsize=16)\n",
    "axs[0].set_xlim(-3,3)\n",
    "axs[0].set_ylim(-0.7, 0.7)\n",
    "axs[1].set_ylim(-0.2, 0.2)\n",
    "axs[0].plot(test_pts[:,0].detach(), prediction.detach(), color=color_pal[0], label=f\"Predicted $\\log r(x)$\")\n",
    "axs[0].fill_between(test_pts[:,0].detach(), (prediction - var.diag()**0.5).detach(), (prediction + var.diag()**0.5).detach(), alpha=0.5, color=color_pal[0])\n",
    "axs[1].fill_between(test_pts[:,0].detach(), (prediction - 0.2*test_pts[:,0] - var.diag()**0.5).detach(), (prediction - 0.2*test_pts[:,0] + var.diag()**0.5).detach(), alpha=0.5, color=color_pal[0])\n",
    "axs[0].plot(test_pts[:,0].detach(), 0.2*test_pts[:,0].detach(), 'k--', label=f\"True $\\log r(x)$\")\n",
    "axs[1].plot(test_pts[:,0].detach(), (prediction - 0.2*test_pts[:,0]).detach())\n",
    "axs[1].axhline(0, color='k', linestyle = '--')\n",
    "axs[0].legend(fontsize=16, frameon=False)\n",
    "axs[0].set_title(f\"Estimated Density Ratio, $M=16$, Bootstrap\", fontsize=20)\n",
    "plt.savefig(\"fig2.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Using \"raw_pred\" because we do not include the bias correction in the main body\n",
    "part_pred_16 = torch.load('YOUR_DIR_HERE/partition_raw_pred_arr.pt')[-1]\n",
    "part_true_16 = torch.load('YOUR_DIR_HERE/partition_true_arr.pt')[-1]\n",
    "part_unc_16 = torch.load('YOUR_DIR_HERE/partition_unc_arr.pt')[-1]\n",
    "\n",
    "# Trained up to M=32 for bootstrap, so need the second to last element for 16\n",
    "strap_pred_16 = torch.load('YOUR_DIR_HERE/bootstrap_raw_pred_arr.pt')[-2]\n",
    "strap_true_16 = torch.load('YOUR_DIR_HERE/bootstrap_true_arr.pt')[-2]\n",
    "strap_unc_16 = torch.load('YOUR_DIR_HERE/bootstrap_unc_arr.pt')[-2]\n",
    "\n",
    "baseline_ensemble_pred_16 = torch.load('YOUR_DIR_HERE/baseline_ensemble_pred_arr.pt')[-1]\n",
    "baseline_ensemble_true_16 = torch.load('YOUR_DIR_HERE/baseline_ensemble_true_arr.pt')[-1]\n",
    "baseline_ensemble_unc_16 = torch.load('YOUR_DIR_HERE/baseline_ensemble_unc_arr.pt')[-1]\n",
    "\n",
    "part_coverages = ((((part_pred_16 - part_true_16).abs()/part_unc_16)).unsqueeze(-1) < sigmas).float().mean(dim=2)\n",
    "strap_coverages = ((((strap_pred_16 - strap_true_16).abs()/strap_unc_16)).unsqueeze(-1) < sigmas).float().mean(dim=2)\n",
    "baseline_coverages = ((((baseline_ensemble_pred_16 - baseline_ensemble_true_16).abs()/baseline_ensemble_unc_16)).unsqueeze(-1) < sigmas).float().mean(dim=2)\n",
    "\n",
    "num_trainings = strap_coverages.shape[1]\n",
    "color_pal = sns.color_palette('bright')\n",
    "mpl.rc('font',family='Times New Roman')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "num_subnet_arr = [16]\n",
    "f_arr = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5]\n",
    "coverages = torch.stack([part_coverages[:4], strap_coverages[:4], baseline_coverages[:4]], dim=-1)[-1,:,:]\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(5, 3, height_ratios=[2.5, 1, 0.5, 2.5, 1], hspace=0.04, wspace=0)\n",
    "\n",
    "axs = np.array([\n",
    "    [fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1]), fig.add_subplot(gs[0, 2])],\n",
    "    [fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1]), fig.add_subplot(gs[1, 2])],\n",
    "    [None, None, None],  # This row is an empty space\n",
    "    [fig.add_subplot(gs[3, 0]), fig.add_subplot(gs[3, 1]), fig.add_subplot(gs[3, 2])],\n",
    "    [fig.add_subplot(gs[4, 0]), fig.add_subplot(gs[4, 1]), fig.add_subplot(gs[4, 2])],\n",
    "])\n",
    "# fig, axs = plt.subplots(4,2,sharex=True,figsize = (16,12),gridspec_kw={'hspace': 0, 'wspace': 0, 'height_ratios': [2.5, 1, 2.5, 1]})\n",
    "axs[0,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[0,1].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[0,2].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[1,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[1,0].tick_params(axis='x', which='major', direction='in', labelsize=0)\n",
    "axs[1,1].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[1,2].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[3,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[3,1].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[3,2].tick_params(axis='both', which='major', direction='in', labelsize=0)\n",
    "axs[4,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[4,1].tick_params(axis='y', which='major', direction='in', labelsize=0)\n",
    "axs[4,1].tick_params(axis='x', which='major', direction='in', labelsize=16)\n",
    "axs[4,2].tick_params(axis='y', which='major', direction='in', labelsize=0)\n",
    "axs[4,2].tick_params(axis='x', which='major', direction='in', labelsize=16)\n",
    "\n",
    "method_names = ['Partition', 'Bootstrap', 'Naive Ensemble']\n",
    "# colors = ['C0', 'C1', '#555555']\n",
    "colors = [color_pal[0], color_pal[1], '#555555']\n",
    "linestyles = ['-', '-', '-.']\n",
    "\n",
    "axs[0,0].set_title(r\"$\\kappa=0.01$\", fontsize=22)\n",
    "axs[0,0].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[1,0].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,0].set_ylabel(r\"$c(z) - \\bar{c}(z)$\", fontsize=18)\n",
    "axs[1,0].set_ylim(-0.05, 0.05)\n",
    "axs[0,0].set_ylabel(r'$c(z)$',fontsize=18)\n",
    "\n",
    "axs[0,1].set_title(r\"$\\kappa=0.02$\", fontsize=22)\n",
    "axs[0,1].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[1,1].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,1].set_ylim(-0.05, 0.05)\n",
    "\n",
    "axs[0,2].set_title(r\"$\\kappa=0.05$\", fontsize=22)\n",
    "axs[0,2].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[1,2].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,2].set_ylim(-0.05, 0.05)\n",
    "\n",
    "axs[3,0].set_title(r\"$\\kappa=0.1$\", fontsize=22)\n",
    "axs[3,0].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[4,0].axhline(y=0, color='k', linestyle='--')\n",
    "axs[4,0].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[4,0].set_ylabel(r\"$c(z) - \\bar{c}(z)$\", fontsize=18)\n",
    "axs[4,0].set_ylim(-0.05, 0.05)\n",
    "axs[3,0].set_ylabel(r'$c(z)$',fontsize=18)\n",
    "\n",
    "axs[3,1].set_title(r\"$\\kappa=0.2$\", fontsize=22)\n",
    "axs[3,1].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[4,1].axhline(y=0, color='k', linestyle='--')\n",
    "axs[4,1].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[4,1].set_ylim(-0.05, 0.05)\n",
    "\n",
    "axs[3,2].set_title(r\"$\\kappa=0.5$\", fontsize=22)\n",
    "axs[3,2].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[4,2].axhline(y=0, color='k', linestyle='--')\n",
    "axs[4,2].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[4,2].set_ylim(-0.05, 0.05)\n",
    "\n",
    "for method_idx_swap in range(3):\n",
    "    if method_idx_swap == 0:\n",
    "        method_idx = 1\n",
    "    elif method_idx_swap == 1:\n",
    "        method_idx = 0\n",
    "    elif method_idx_swap == 2:\n",
    "        method_idx = 2\n",
    "    for f_idx, f in enumerate(f_arr):\n",
    "        if f_idx == 0:\n",
    "            axs[0,0].plot(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[0,0].fill_between(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[:,f_idx,:,method_idx].mean(axis=0) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            axs[1,0].plot(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[1,0].fill_between(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "        elif f_idx == 1:\n",
    "            axs[0,1].plot(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[0,1].fill_between(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[:,f_idx,:,method_idx].mean(axis=0) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            axs[1,1].plot(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:    \n",
    "                axs[1,1].fill_between(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "        elif f_idx == 2:\n",
    "            axs[0,2].plot(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[0,2].fill_between(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[:,f_idx,:,method_idx].mean(axis=0) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            axs[1,2].plot(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[1,2].fill_between(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "        elif f_idx == 3:\n",
    "            axs[3,0].plot(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[3,0].fill_between(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[:,f_idx,:,method_idx].mean(axis=0) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            axs[4,0].plot(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[4,0].fill_between(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "        elif f_idx == 4:\n",
    "            axs[3,1].plot(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[3,1].fill_between(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[:,f_idx,:,method_idx].mean(axis=0) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            axs[4,1].plot(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[4,1].fill_between(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "        elif f_idx == 5:\n",
    "            axs[3,2].plot(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[3,2].fill_between(sigmas, coverages[:,f_idx,:,method_idx].mean(axis=0) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, coverages[:,f_idx,:,method_idx].mean(axis=0) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            axs[4,2].plot(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages), label=f'{method_names[method_idx]}', color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "            if method_idx != 2:\n",
    "                axs[4,2].fill_between(sigmas, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) - coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, (coverages[:,f_idx,:,method_idx].mean(axis=0) - ideal_coverages) + coverages[:,f_idx,:,method_idx].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[method_idx], linestyle=linestyles[method_idx])\n",
    "\n",
    "axs[0,0].legend(fontsize=16, frameon=False)\n",
    "axs[0,1].legend(fontsize=16, frameon=False)\n",
    "axs[0,2].legend(fontsize=16, frameon=False)\n",
    "axs[3,0].legend(fontsize=16, frameon=False)\n",
    "axs[3,1].legend(fontsize=16, frameon=False)\n",
    "axs[3,2].legend(fontsize=16, frameon=False)\n",
    "plt.suptitle(r\"Gaussian Case Study $M=16$, Coverage on Mixture Fraction $\\kappa$\", fontsize=24)\n",
    "plt.savefig(\"fig3.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# We take M=8, 16, and 32 for this study\n",
    "bootstrap_pred = torch.load('YOUR_DIR_HERE/bootstrap_pred_arr.pt')[2:]\n",
    "bootstrap_raw_pred = torch.load('YOUR_DIR_HERE/bootstrap_raw_pred_arr.pt')[2:]\n",
    "bootstrap_true = torch.load('YOUR_DIR_HERE/bootstrap_true_arr.pt')[2:]\n",
    "bootstrap_unc = torch.load('YOUR_DIR_HERE/bootstrap_unc_arr.pt')[2:]\n",
    "\n",
    "bootstrap_pred_8 = bootstrap_pred[0]\n",
    "bootstrap_true_8 = bootstrap_true[0]\n",
    "bootstrap_unc_8 = bootstrap_unc[0]\n",
    "bootstrap_raw_pred_8 = bootstrap_raw_pred[0]\n",
    "\n",
    "bootstrap_pred_16 = bootstrap_pred[1]\n",
    "bootstrap_true_16 = bootstrap_true[1]\n",
    "bootstrap_unc_16 = bootstrap_unc[1]\n",
    "bootstrap_raw_pred_16 = bootstrap_raw_pred[1]\n",
    "\n",
    "bootstrap_pred_32 = bootstrap_pred[2]\n",
    "bootstrap_true_32 = bootstrap_true[2]\n",
    "bootstrap_unc_32 = bootstrap_unc[2]\n",
    "bootstrap_raw_pred_32 = bootstrap_raw_pred[2]\n",
    "\n",
    "num_subnet_arr = [2, 4, 8, 16, 32]\n",
    "sigmas = torch.linspace(0,5,500)\n",
    "ideal_coverages = norm.cdf(sigmas) - norm.cdf(-sigmas)\n",
    "i=3\n",
    "color_pal = sns.color_palette('bright')\n",
    "colors = [color_pal[5], color_pal[4]]\n",
    "mpl.rc('font',family='Times New Roman')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "\n",
    "bootstrap_z_scores_8 = ((bootstrap_pred_8 - bootstrap_true_8)/bootstrap_unc_8)\n",
    "bootstrap_coverages_8 = (bootstrap_z_scores_8.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)[:,:,i,:]\n",
    "\n",
    "bootstrap_z_scores_16 = ((bootstrap_pred_16 - bootstrap_true_16)/bootstrap_unc_16)\n",
    "bootstrap_coverages_16 = (bootstrap_z_scores_16.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)[:,:,i,:]\n",
    "\n",
    "bootstrap_z_scores_32 = ((bootstrap_pred_32 - bootstrap_true_32)/bootstrap_unc_32)\n",
    "bootstrap_coverages_32 = (bootstrap_z_scores_32.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)[:,:,i,:]\n",
    "\n",
    "bootstrap_raw_z_scores_8 = ((bootstrap_raw_pred_8 - bootstrap_true_8)/bootstrap_unc_8)\n",
    "bootstrap_raw_coverages_8 = (bootstrap_raw_z_scores_8.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)[:,:,i,:]\n",
    "\n",
    "bootstrap_raw_z_scores_16 = ((bootstrap_raw_pred_16 - bootstrap_true_16)/bootstrap_unc_16)\n",
    "bootstrap_raw_coverages_16 = (bootstrap_raw_z_scores_16.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)[:,:,i,:]\n",
    "\n",
    "bootstrap_raw_z_scores_32 = ((bootstrap_raw_pred_32 - bootstrap_true_32)/bootstrap_unc_32)\n",
    "bootstrap_raw_coverages_32 = (bootstrap_raw_z_scores_32.abs().unsqueeze(-1) < sigmas).float().mean(dim=2)[:,:,i,:]\n",
    "\n",
    "num_trainings = bootstrap_coverages_32.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15,5), sharey=True, gridspec_kw={'wspace': 0})\n",
    "i=3\n",
    "color_pal = sns.color_palette('bright')\n",
    "mpl.rc('font',family='Times New Roman')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "colors = [color_pal[5], color_pal[4]]\n",
    "axs[0].set_ylabel(\"Count\", fontsize=18)\n",
    "axs[0].hist((bootstrap_pred_8 - bootstrap_true_8)[0,:,:,i].flatten().numpy(), bins='auto', alpha=0.5, label=r\"$\\hat{\\kappa}_{\\rm BC}$\", color=colors[0])\n",
    "axs[0].hist((bootstrap_raw_pred_8 - bootstrap_true_8)[0,:,:,i].flatten().numpy(), bins='auto', alpha=0.5, label=r\"$\\hat{\\kappa}$\", color=colors[1])\n",
    "axs[0].set_xlabel(\"Residual\", fontsize=18)\n",
    "axs[0].axvline(0, color='k', linestyle='--')\n",
    "axs[0].axvline((bootstrap_pred_8 - bootstrap_true_8)[0,:,:,i].flatten().numpy().mean(), color=colors[0])\n",
    "axs[0].axvline((bootstrap_raw_pred_8 - bootstrap_true_8)[0,:,:,i].flatten().numpy().mean(), color=colors[1])\n",
    "axs[0].set_title(\"M=8 residuals\", fontsize=22)\n",
    "axs[0].legend(fontsize = 16, frameon=False)\n",
    "axs[0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[0].set_xlim(-0.15, 0.15)\n",
    "\n",
    "axs[1].hist((bootstrap_pred_16 - bootstrap_true_16)[0,:,:,i].flatten().numpy(), bins='auto', alpha=0.5, label=r\"$\\hat{\\kappa}_{\\rm BC}$\", color=colors[0])\n",
    "axs[1].hist((bootstrap_raw_pred_16 - bootstrap_true_16)[0,:,:,i].flatten().numpy(), bins='auto', alpha=0.5, label=r\"$\\hat{\\kappa}$\", color=colors[1])\n",
    "axs[1].set_xlabel(\"Residual\", fontsize=18)\n",
    "axs[1].axvline(0, color='k', linestyle='--')\n",
    "axs[1].axvline((bootstrap_pred_16 - bootstrap_true_16)[0,:,:,i].flatten().numpy().mean(), color=colors[0])\n",
    "axs[1].axvline((bootstrap_raw_pred_16 - bootstrap_true_16)[0,:,:,i].flatten().numpy().mean(), color=colors[1])\n",
    "axs[1].set_title(\"M=16 residuals\", fontsize=22)\n",
    "axs[1].legend(fontsize = 16, frameon=False)\n",
    "axs[1].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[1].set_xlim(-0.15, 0.15)\n",
    "\n",
    "axs[2].hist((bootstrap_pred_32 - bootstrap_true_32)[0,:,:,i].flatten().numpy(), bins='auto', alpha=0.5, label=r\"$\\hat{\\kappa}_{\\rm BC}$\", color=colors[0])\n",
    "axs[2].hist((bootstrap_raw_pred_32 - bootstrap_true_32)[0,:,:,i].flatten().numpy(), bins='auto', alpha=0.5, label=r\"$\\hat{\\kappa}$\", color=colors[1])\n",
    "axs[2].set_xlabel(\"Residual\", fontsize=18)\n",
    "axs[2].axvline(0, color='k', linestyle='--')\n",
    "axs[2].axvline((bootstrap_pred_32 - bootstrap_true_32)[0,:,:,i].flatten().numpy().mean(), color=colors[0])\n",
    "axs[2].axvline((bootstrap_raw_pred_32 - bootstrap_true_32)[0,:,:,i].flatten().numpy().mean(), color=colors[1])\n",
    "axs[2].set_title(\"M=32 residuals\", fontsize=22)\n",
    "axs[2].legend(fontsize = 16, frameon=False)\n",
    "axs[2].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[2].set_xlim(-0.15, 0.15)\n",
    "plt.suptitle(r\"Gaussian Case Study, Uncorrected and Corrected Residuals on $\\kappa=0.1$\", fontsize=24, y=1.05)\n",
    "plt.savefig(\"fig7.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3, sharex=\"col\", figsize = (14,6), sharey=\"row\", gridspec_kw={'hspace': 0.05, 'wspace' : 0.04, 'height_ratios': [2, 1]})\n",
    "axs[0,2].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[1,2].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[0,2].set_xlim(0, 5)\n",
    "axs[0,2].set_ylim(0, 1)\n",
    "subnet_idx = 0\n",
    "axs[0,2].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "\n",
    "axs[0,2].plot(sigmas, bootstrap_coverages_32[subnet_idx,:,:].mean(axis=0), label=f'Bias correction', color=colors[0])\n",
    "axs[0,2].plot(sigmas, bootstrap_raw_coverages_32[subnet_idx,:,:].mean(axis=0), label=f'No bias correction', color=colors[1])\n",
    "axs[0,2].fill_between(sigmas, bootstrap_coverages_32[subnet_idx,:,:].mean(axis=0) - bootstrap_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, bootstrap_coverages_32[subnet_idx,:,:].mean(axis=0) + bootstrap_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[0])\n",
    "axs[0,2].fill_between(sigmas, bootstrap_raw_coverages_32[subnet_idx,:,:].mean(axis=0) - bootstrap_raw_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, bootstrap_raw_coverages_32[subnet_idx,:,:].mean(axis=0) + bootstrap_raw_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[1])\n",
    "axs[1,2].plot(sigmas, (bootstrap_coverages_32[subnet_idx,:,:].mean(axis=0) - ideal_coverages), label=f'Bias correction', color=colors[0])\n",
    "axs[1,2].plot(sigmas, (bootstrap_raw_coverages_32[subnet_idx,:,:].mean(axis=0) - ideal_coverages), label=f'No bias correction', color=colors[1])\n",
    "axs[1,2].fill_between(sigmas, (bootstrap_coverages_32[subnet_idx,:,:].mean(axis=0) - ideal_coverages) - bootstrap_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, (bootstrap_coverages_32[subnet_idx,:,:].mean(axis=0) - ideal_coverages) + bootstrap_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[0])\n",
    "axs[1,2].fill_between(sigmas, (bootstrap_raw_coverages_32[subnet_idx,:,:].mean(axis=0) - ideal_coverages) - bootstrap_raw_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, (bootstrap_raw_coverages_32[subnet_idx,:,:].mean(axis=0) - ideal_coverages) + bootstrap_raw_coverages_32[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[1])\n",
    "axs[1,2].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,2].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[1,0].set_ylabel(r\"$c(z) - \\bar{c}(z)$\", fontsize=18)\n",
    "axs[0,2].set_ylim(0, 1)\n",
    "axs[1,2].set_ylim(-0.3, 0.3)\n",
    "axs[0,0].set_ylabel(r'$c(z)$',fontsize=18)\n",
    "axs[0,2].set_title('M=32',fontsize=22)\n",
    "axs[0,2].legend(fontsize=16, frameon=False)\n",
    "\n",
    "axs[0,1].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[1,1].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[0,1].set_xlim(0, 5)\n",
    "axs[0,1].set_ylim(0, 1)\n",
    "subnet_idx = 0\n",
    "axs[0,1].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[0,1].plot(sigmas, bootstrap_coverages_16[subnet_idx,:,:].mean(axis=0), label=f'Bias correction', color=colors[0])\n",
    "axs[0,1].plot(sigmas, bootstrap_raw_coverages_16[subnet_idx,:,:].mean(axis=0), label=f'No bias correction', color=colors[1])\n",
    "axs[0,1].fill_between(sigmas, bootstrap_coverages_16[subnet_idx,:,:].mean(axis=0) - bootstrap_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, bootstrap_coverages_16[subnet_idx,:,:].mean(axis=0) + bootstrap_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[0])\n",
    "axs[0,1].fill_between(sigmas, bootstrap_raw_coverages_16[subnet_idx,:,:].mean(axis=0) - bootstrap_raw_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, bootstrap_raw_coverages_16[subnet_idx,:,:].mean(axis=0) + bootstrap_raw_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[1])\n",
    "axs[1,1].plot(sigmas, (bootstrap_coverages_16[subnet_idx,:,:].mean(axis=0) - ideal_coverages), label=f'Bias correction', color=colors[0])\n",
    "axs[1,1].plot(sigmas, (bootstrap_raw_coverages_16[subnet_idx,:,:].mean(axis=0) - ideal_coverages), label=f'No bias correction', color=colors[1])\n",
    "axs[1,1].fill_between(sigmas, (bootstrap_coverages_16[subnet_idx,:,:].mean(axis=0) - ideal_coverages) - bootstrap_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, (bootstrap_coverages_16[subnet_idx,:,:].mean(axis=0) - ideal_coverages) + bootstrap_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[0])\n",
    "axs[1,1].fill_between(sigmas, (bootstrap_raw_coverages_16[subnet_idx,:,:].mean(axis=0) - ideal_coverages) - bootstrap_raw_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, (bootstrap_raw_coverages_16[subnet_idx,:,:].mean(axis=0) - ideal_coverages) + bootstrap_raw_coverages_16[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[1])\n",
    "\n",
    "axs[1,1].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,1].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[0,1].set_ylim(0, 1)\n",
    "axs[1,1].set_ylim(-0.3, 0.3)\n",
    "axs[0,1].set_title('M=16',fontsize=22)\n",
    "axs[0,1].legend(fontsize=16, frameon=False)\n",
    "\n",
    "axs[0,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[1,0].tick_params(axis='both', which='major', direction='in', labelsize=16)\n",
    "axs[0,0].set_xlim(0, 5)\n",
    "axs[0,0].set_ylim(0, 1)\n",
    "subnet_idx = 0\n",
    "axs[0,0].plot(sigmas, ideal_coverages, 'k--', label='Ideal')\n",
    "axs[0,0].plot(sigmas, bootstrap_coverages_8[subnet_idx,:,:].mean(axis=0), label=f'Bias correction', color=colors[0])\n",
    "axs[0,0].plot(sigmas, bootstrap_raw_coverages_8[subnet_idx,:,:].mean(axis=0), label=f'No bias correction', color=colors[1])\n",
    "axs[0,0].fill_between(sigmas, bootstrap_coverages_8[subnet_idx,:,:].mean(axis=0) - bootstrap_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, bootstrap_coverages_8[subnet_idx,:,:].mean(axis=0) + bootstrap_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[0])\n",
    "axs[0,0].fill_between(sigmas, bootstrap_raw_coverages_8[subnet_idx,:,:].mean(axis=0) - bootstrap_raw_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, bootstrap_raw_coverages_8[subnet_idx,:,:].mean(axis=0) + bootstrap_raw_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[1])\n",
    "axs[1,0].plot(sigmas, (bootstrap_coverages_8[subnet_idx,:,:].mean(axis=0) - ideal_coverages), label=f'Bias correction', color=colors[0])\n",
    "axs[1,0].plot(sigmas, (bootstrap_raw_coverages_8[subnet_idx,:,:].mean(axis=0) - ideal_coverages), label=f'No bias correction', color=colors[1])\n",
    "axs[1,0].fill_between(sigmas, (bootstrap_coverages_8[subnet_idx,:,:].mean(axis=0) - ideal_coverages) - bootstrap_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, (bootstrap_coverages_8[subnet_idx,:,:].mean(axis=0) - ideal_coverages) + bootstrap_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[0])\n",
    "axs[1,0].fill_between(sigmas, (bootstrap_raw_coverages_8[subnet_idx,:,:].mean(axis=0) - ideal_coverages) - bootstrap_raw_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, (bootstrap_raw_coverages_8[subnet_idx,:,:].mean(axis=0) - ideal_coverages) + bootstrap_raw_coverages_8[subnet_idx,:,:].std(axis=0)/num_trainings**0.5, alpha=0.5, color=colors[1])\n",
    "axs[1,0].axhline(y=0, color='k', linestyle='--')\n",
    "axs[1,0].set_xlabel(f'$z$', fontsize=18)\n",
    "axs[0,0].set_ylim(0, 1)\n",
    "axs[1,0].set_ylim(-0.3, 0.3)\n",
    "axs[0,0].set_title('M=8',fontsize=22)\n",
    "axs[0,0].legend(fontsize=16, frameon=False)\n",
    "# plt.show()\n",
    "plt.suptitle(r\"Gaussian Case Study, Uncorrected and Corrected Coverage for $\\kappa=0.1$\", fontsize=24, y=1.05)\n",
    "plt.savefig(\"fig8.pdf\", bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
